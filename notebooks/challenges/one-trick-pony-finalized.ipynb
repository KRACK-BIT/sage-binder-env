{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d51d4cc1-546a-4904-a897-2e121d1a22f9",
   "metadata": {},
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9dced3-e13f-4d18-aa54-9c4f185f0122",
   "metadata": {},
   "outputs": [],
   "source": [
    "%display latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601caee0-4cfa-4892-8840-e5157fd6bb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pycryptodome pwntools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01001b0-988e-4e03-a01b-52ce06906507",
   "metadata": {},
   "source": [
    "# Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d60fff6-70b8-477f-ba01-a15ed1d7052c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib, json\n",
    "from Crypto.Util.number import getPrime, isPrime\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bf526f-5691-44f3-aa2a-482221d1d5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isPrime(FROST_PRIME := 0x1a66804d885939d7acf3a4b413c9a24547b876e706913adec9684cc4a63ab0dfd2e0fd79f683de06ad17774815dfc8375370eb3d0fb5dce0019bd0632e7663a41)\n",
    "LIMIT = 2_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c3e1e4-6d96-4a45-b383-316971ee719b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinselRNG:\n",
    "    def __init__(self, bits):\n",
    "        self.p = getPrime(bits)\n",
    "        self.g = randint(1, self.p)\n",
    "\n",
    "    def sparkle_bit(self):\n",
    "        if self.g == 0:\n",
    "            self.g += 1\n",
    "        while True:\n",
    "            shimmer = pow(self.g, (self.p-1)//2, self.p)\n",
    "            yield int(shimmer == 1)\n",
    "            self.g = (self.g + 1) % self.p\n",
    "            if self.g == 0:\n",
    "                self.g += 1\n",
    "\n",
    "    def gather_sparkles(self, l):\n",
    "        bits = ''\n",
    "        for i, b in enumerate(self.sparkle_bit()):\n",
    "            if i == l: break\n",
    "            bits += str(b)\n",
    "        return int(bits, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df5d3e1-cd13-476b-b214-1664430465c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frostscribe_signature(msg):\n",
    "    blizzard = hashlib.sha512(msg.encode()).digest()\n",
    "    snowmark = int.from_bytes(blizzard, \"big\") % FROST_PRIME\n",
    "    lantern_key = frostrng.gather_sparkles(500)\n",
    "    etch = pow(snowmark, lantern_key, FROST_PRIME)\n",
    "    return {\"signature\": str(etch)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f76e1f-769b-426b-b6b9-a77cf32281c7",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b631e2b1-0209-47c5-9c78-6421cd9b6197",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ec9830-c78b-4d0e-8cf8-78840f09b676",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_SIZE = 500\n",
    "KEY_SUBMISSION_SIZE = 84*8\n",
    "\n",
    "WITH_HEAVY_ASSERTS = True\n",
    "TEST_SERVER_PATH = \"debug-server.py\"\n",
    "\n",
    "FALSE_FLAG = \"HTB{fake_flag_for_testing}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8dfe66-1304-4f27-ba83-452e98ea28dd",
   "metadata": {},
   "source": [
    "## Weakness 1: Order of `FROST_ORDER` is smooth as butter\n",
    "\n",
    "Sagemath will crunch these discrete logarithms of $g^x \\equiv b \\bmod (F \\coloneqq \\text{FROST PRIME})$ quickly via pohlig-hellman.\n",
    "\n",
    "### 'Smooth' group order: decomposable into small prime factors\n",
    "- Exists in contrast to safe prime/sophie-germaine prime pairs of $(p,q)$, where $p - 1 = 2q$ (think of these as having a extraordinarily 'jagged' order ‚õ∞Ô∏è - ergo, why these are so useful in cryptography, and why using your own primes w/o checking can shoot yourself in the foot!)\n",
    "\n",
    "### Pohlig-hellman\n",
    "\n",
    "TL;DR is for composite group orders, to augment your generic strategy with an initial a CRT-style decomposition to gain divide-and-conquer benefits\n",
    "\n",
    "The step-by-step under-the-hood is:\n",
    "- CRT-decompose `FROST_ORDER` into its prime-power subgroups\n",
    "    - easily done with $g^x \\equiv b \\bmod F \\implies (h_i \\coloneqq g^\\frac{F - 1}{{p_i}^{e_i}})^x$ $ \\equiv b^\\frac{F - 1}{{p_i}^{e_i}} \\bmod F$\n",
    "    - Note that these sub-cases of $(h_i)^x$, only have sub-order ${p_i}^{e_i}$ (not $F - 1$)\n",
    "    - Thus, we are now dealing with equations not of $x \\mod F - 1$, but its sub-cycles of $x_i \\bmod {p_i}^{e_i}$ \n",
    "- Solve these smaller sub-cycle cases with a generic cyclic search strategy, such as:\n",
    "    - Exhaustive search (self-explanatory)\n",
    "    - Baby-step Giant-step (meet-in-the-middle table go brrr)\n",
    "    - Pollard's rho (still the üêê of generic search strategies)\n",
    "    - Pollard's lambda/ü¶ò (variant of rho, where you can control the search bounds)\n",
    "- Then, recombine their results via CRT to get $x \\bmod F - 1$!\n",
    "    - The set of equations of $x_i \\bmod {p_i}^{e_i}$ uniquely identify $x \\bmod F - 1$\n",
    "    - Recombination here is literally just the textbook CRT\n",
    "    - Ergo, calculate your CRT basis for each ${p_i}^{e_i}$, scale by $x_i$, and you will get $x \\bmod \\prod_i {p_i}^{e_i}$, i.e. $x \\bmod F - 1$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d0f7b3-ab24-4187-846b-970ad4e20451",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(FROST_ORDER := factor(FROST_PRIME - 1), ceil(log(FROST_PRIME - 1, 2)))\n",
    "assert max(p for p, e in FROST_ORDER) < 1 << 16\n",
    "ZP_FROST = Zmod(FROST_PRIME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ac5ec9-7030-4767-9344-feee18e37718",
   "metadata": {},
   "source": [
    "## Weakness 2: Linear Legendre PRG has small state + seeds via secret offset + progresses sequentially"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ec24cf-78c2-4903-b6c6-5c9a264eff5a",
   "metadata": {},
   "source": [
    "Linear Legrendre PRF/PRG is based on:\n",
    "- Public parameters: $p$ (our prime field)\n",
    "    - Public sequence: $L_p : (\\mathbb{Z}/p\\mathbb{Z})^* \\to \\{-1,1\\} : i \\mapsto \\left(\\dfrac{i}{p}\\right)_\\text{Leg}$\n",
    "    - We then remap our output per $\\{-1 \\mapsto 0, 1 \\mapsto 1\\}$\n",
    "- Private parameters $k$ (our secret offset)\n",
    "    - Seed: $k \\mapsto i \\mapsto L_p(i + k)$\n",
    "\n",
    "For this, we are just going to use a simple meet-in-the-middle strategy. This exploits for this PRNG, the sequence is public; just the offset is hidden. Thus, if your guess falls within the RNG stream, you can recover both the original seed and current state trivially by rewinding/progressing your index with a corresponding offset.\n",
    "\n",
    "Thus, you can 'meet-in-the-middle' by finding overlapping candidates between the oracle RNG-stream substrings, and guessed-offset RNG-stream substrings.\n",
    "- Then, validate each candidate by checking if their inferred seed-offset would generate the full oracle RNG-stream\n",
    "\n",
    "Thus, this solution consists of:\n",
    "\n",
    "1. Extract continuous PRG bit stream material from repeated signature queries\n",
    "    - Since we know the message, we know $g$, and since $F$ is weak, we can discrete-log out the lantern key used\n",
    "    - Ideally, we would get ~$\\sqrt{p}$ bits of RNG material; but practically, we just get as much as possible\n",
    "\n",
    "2. Create a lookup table to validate candidates against:\n",
    "    - Keys: all $n$-length continuous substring tuples in the PRNG bit stream\n",
    "    - Values: the substring offset (i.e. whatever index-arithmetic this substring has to the seed offset $k$)\n",
    "        - NOTE: all offsets must have a unique substring; thus, minimum length is ~$\\lceil \\log_2(p) \\rceil$\n",
    "        - As such, redo with ($n \\coloneqq n+1$)-length substrings if collision found, until minimum collision-free substring dictionary is found\n",
    "\n",
    "3. Sample candidate $L_p$ substrings with random-offsets $r$ and length $n$, and check if $\\|_{i=0}^{n-1} L_p(r + i)$ matches a substring in the lookup table\n",
    "\n",
    "    - $O(1)$ hash-map makes initial pruning of offset guesses fast and independent from how much RNG material we collected\n",
    "    - If a match is found, then derive the inferred $k^\\prime$ offset, and check if $k^\\prime$ yields the same full RNG-stream as observed\n",
    "        - If no - go back to guessing\n",
    "        - If yes - boom, we have recoverd $k$!\n",
    "\n",
    "4. Now, we can solve any RNG challenges we need to get the flag!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1883c73d-8146-456f-b311-918b44c6ddd1",
   "metadata": {},
   "source": [
    "### Weakness 2.5: Legrendre is multiplicative - thus, can derive $M^2$ solution offsets from a $M$-length sequential RNG stream"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88035d8e-fac7-4d34-b318-91dec63fb248",
   "metadata": {},
   "source": [
    "Interest in the Legrendre PRF recently spiked when the etherium foundation started exploring using the Legrendre in their protocols. Since then, a ton of optimized breaking strategies on this have been researched and published:\n",
    "\n",
    "- <https://eprint.iacr.org/2020/098.pdf>\n",
    "    - Note pages 5-8 for a direct outline of improvements applicable for this challenge\n",
    "- <https://eprint.iacr.org/2019/1357.pdf>\n",
    " \n",
    "These improvements mainly consist of ways to extract more substring + index-relations from a sequential Legrendre sequence:\n",
    "- This is done by using the Legrendre symbol's multiplicative properties to infer, given a length $M$ RNG-stream value, substrings at:\n",
    "    - $k + i$ offset with length $M - i$\n",
    "        - This is just the seeded-by-offset weakness, expressed formally as $\\left(\\dfrac{(k + \\epsilon) + i}{p}\\right)_\\text{Leg} = \\left(\\dfrac{k + (i + \\epsilon)}{p}\\right)_\\text{Leg}$\n",
    "        - i.e. if we guess $k + \\epsilon \\land \\epsilon \\leq M$, we match to a substring, and derive the candidate per $k = (k + \\epsilon) - \\epsilon$\n",
    "    - $-k$ offset with length $M$\n",
    "        - per $\\left(\\dfrac{k + i}{p}\\right)_\\text{Leg} \\left(\\dfrac{-1}{p}\\right)_\\text{Leg} = \\left(\\dfrac{-k - i}{p}\\right)_\\text{Leg}$\n",
    "        - i.e. Legrendre reflects across $\\mathbb{Z} / p\\mathbb{Z}$ by multiplication with $\\left(\\dfrac{-1}{p}\\right)_\\text{Leg}$; so, can likewise reflect Legrendre sequences to get their negation\n",
    "    - $\\frac{k}{d}$ offsets with length $\\lfloor \\frac{M - 1}{d} \\rfloor$\n",
    "        - per $\\left(\\dfrac{\\frac{k}{d} + i}{p}\\right)_\\text{Leg} \\left(\\dfrac{d}{p}\\right)_\\text{Leg} = \\left(\\dfrac{k + di}{p}\\right)_\\text{Leg}$\n",
    "        - i.e. linear subsequences of $(k + \\epsilon) + di$ within $k + i$ can be rescaled into sequential sequences of $\\frac{k + \\epsilon}{d} + i$\n",
    "\n",
    "Thus, we can beef up our substring dictionary with more matches and candidate relationships to $k$\n",
    "- Not only is now the success chance of each guess increased\n",
    "- But the rate at which guesses improve grows quadratically, not linearly\n",
    "\n",
    "Then, you can also likewise augment the search by not guessing singular sub-strings, but rather guessing larger RNG stream chunks of size $N$.\n",
    "- Then, you can yield $O(N^2)$ substrings guesses w/o any new Legrendre-symbol calls\n",
    "- However, substrings of $N$ are interdependent - pages 7-8 cover decomposition strategies of $N$ into guessesthat try to avoid computing overlapping comparisons\n",
    "    - e.g. don't both negating across $\\mathbb{Z} / p\\mathbb{Z}$, skip implicitly-eliminated shifts, scale by primes to avoid overlapping rescaling factors\n",
    "\n",
    "However, all of this is overkill for the challenge, so will leave this as a fun TODO for later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd011b8-a47a-457b-ae50-70540fa7b3aa",
   "metadata": {},
   "source": [
    "## RNG-stream generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eb7fe4-0ab6-4354-aa04-f080989bfee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_rand_offsets(p):\n",
    "    Zpp = GF(p)\n",
    "    while True:\n",
    "        b = Zpp.random_element()\n",
    "        if not b.is_zero():        \n",
    "            yield b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a678f257-8f12-4855-bc33-2c7dfe4ff9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_rng_stream(p, offset, size, backwards=False):\n",
    "    p = int(p)\n",
    "    offset = int(offset)\n",
    "    size = int(size)\n",
    "    \n",
    "    yield from ((\n",
    "            max(legendre_symbol(idx, p), 0)\n",
    "            if ((idx := 1 + ((offset + (-i if backwards else i) - 1) % (p - 1))) != 0) \n",
    "            else ValueError(\n",
    "                \"Error in rng_stream - this is strange, since my offset arithmetic *should* mean zero never occurs\"\n",
    "                f\"\\n{i=}\\n{idx=}\\n\\n{p=}\\n{offset=}\\n{size=}\"\n",
    "        )) for i in range(size)\n",
    "    )\n",
    "\n",
    "def rng_stream_to_int(it):\n",
    "    state = int(0)\n",
    "    for b in it:\n",
    "        state = state << int(1)\n",
    "        state += int(b)\n",
    "    return state\n",
    "\n",
    "def int_to_rng_stream(i, size):\n",
    "    return ZZ(i).digits(2, padto=size)[::-1]\n",
    "\n",
    "def gen_rng_int(p, offset, size):\n",
    "    return rng_stream_to_int(gen_rng_stream(p, offset, size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236df023-93a9-4ce6-9ce5-d087d6aae5a3",
   "metadata": {},
   "source": [
    "### Asserts to check rng_stream generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a957939-6f99-4488-aec5-3109a18ac0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are a bunch of equivalent methods for computing the internal legendre state\n",
    "# this shows all the ones I explored, and that they match the Tinsel RNG bit-by-bit\n",
    "_assert_rng = TinselRNG(16) # 64\n",
    "\n",
    "_tries = 1 << (\n",
    "    18 # Using tries > 4 * |p|, so slices wrapping around $p$ to $1$ are tested\n",
    "    if WITH_HEAVY_ASSERTS\n",
    "    else 8\n",
    ")\n",
    "_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5545ac33-af17-4c0e-908f-c6818dfd9f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "_start_g = _assert_rng.g\n",
    "assert all((\n",
    "    _assert_rng.gather_sparkles(1)\n",
    "    == max(legendre_symbol(_offset, _assert_rng.p), 0)\n",
    "    == max(kronecker(_offset, _assert_rng.p), 0)\n",
    "    == int(Zmod(_assert_rng.p)(_offset).is_square())\n",
    "    ) for i in tqdm(range(_tries), desc=\"Validating legendre rng_stream methods\")\n",
    "    if (_offset := (_start_g + i) % _assert_rng.p) != 0\n",
    ")\n",
    "del _start_g, _offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e16d4f0-f347-4e70-8e45-98e42dba5c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "_start_g = _assert_rng.g\n",
    "assert all((\n",
    "    _assert_rng.gather_sparkles(_size)\n",
    "    == rng_stream_to_int(_k)\n",
    "    == int(''.join(str(b) for b in _k), 2)\n",
    "    == ZZ(_k[::-1], 2)\n",
    "    ) for i in tqdm(range(_tries), desc=\"Validating rng_stream -> int methods\")\n",
    "    if (_k := tuple(gen_rng_stream(_assert_rng.p, _start_g + (i * _size), _size))) is not None\n",
    "), f\"{_assert_rng}\\n{_assert_rng.p}\\n{_assert_rng.g}\\n\\n{i=}\\n{type(i)=}\\n{_k=}\"\n",
    "del _start_g, _k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecce37f-6388-4314-b29b-bdc38d909efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "_start_g = _assert_rng.g\n",
    "assert all(\n",
    "    _assert_rng.gather_sparkles(_size) == (_k := gen_rng_int(_assert_rng.p, _start_g + (i * _size), _size))\n",
    "    for i in tqdm(range(_tries), desc=\"Validating rng_int method\")\n",
    "), f\"{_assert_rng}\\n{_assert_rng.p}\\n{_assert_rng.g}\\n\\n{i=}\\n{type(i)=}\\n{_k=}\"\n",
    "del _start_g, _k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfdc8e3-b448-4f49-b158-85ba86feee2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all((\n",
    "    r == rng_stream_to_int(int_to_rng_stream(r, _size))\n",
    "    ) for r in tqdm(range(1 << _size), desc=\"Validating int -> rng_stream methods\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b922ad-998d-428a-8c1b-ebdf906e9950",
   "metadata": {},
   "outputs": [],
   "source": [
    "del _assert_rng, _tries, _size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e3e850-aff4-4e30-8e5a-506141d804b5",
   "metadata": {},
   "source": [
    "## Substring Dictionary Builders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3d0dc6-313a-491c-8674-a8bd3b2d15c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tqdm_remover(bar):\n",
    "    # <https://stackoverflow.com/a/79707132>\n",
    "    # bar.container.close()\n",
    "    bar.leave = False\n",
    "    bar.n = bar.total\n",
    "    bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92f15ec-fb30-4055-b569-3a89f5040476",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from itertools import islice\n",
    "\n",
    "def sliding_window(iterable, n):\n",
    "    it = iter(iterable)\n",
    "    d = deque(islice(it, n), maxlen=int(n))\n",
    "    if len(d) == n:\n",
    "        yield tuple(d)\n",
    "    for x in it:\n",
    "        d.append(x)\n",
    "        yield tuple(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a8b74e-d232-4212-ab4e-21ff5550aae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_substring_dict(it, *, max_len=128, clear_bar=True):\n",
    "    seq = tuple(it)\n",
    "    num_states = len(set(seq))\n",
    "\n",
    "    min_len = int(ceil(log(len(seq), num_states))) # ~approx pigenhole principle for seq to be feasible\n",
    "\n",
    "    for n in tqdm(\n",
    "        range(min_len, max_len + 1),\n",
    "        desc=\"Trialling substring lengths for unique index dictionary\",\n",
    "        initial=int(min_len),\n",
    "        total=int(max_len),\n",
    "        # position=0\n",
    "    ):\n",
    "        subseq_dict = {}\n",
    "        for idx, subseq in (inner_tqdm := tqdm(\n",
    "            enumerate(sliding_window(seq, n)),\n",
    "            desc=f\"Trial - {n=}\",\n",
    "            initial=int(n-1),\n",
    "            total=int(len(seq)),\n",
    "            # position=1,\n",
    "            # leave=False,\n",
    "        )):\n",
    "            if subseq in subseq_dict:\n",
    "                if clear_bar: tqdm_remover(inner_tqdm)\n",
    "                break\n",
    "            subseq_dict[subseq] = idx\n",
    "        else:\n",
    "            assert len(seq) == (n + len(subseq_dict) - 1)\n",
    "            assert all(v == seq[i:i+n] for v, i in subseq_dict.items())\n",
    "            return n, subseq_dict\n",
    "    return None, None\n",
    "\n",
    "def subint_dict(subseq_dict):\n",
    "    return {rng_stream_to_int(k): v for k, v in tqdm(subseq_dict.items(), desc=\"parsing_to_int\")}\n",
    "\n",
    "# def get_unique_subint_dict(it, **kargs):\n",
    "#     n, subseq_dict = get_unique_substring_dict(it, **kargs)\n",
    "#     return n, subint_dict(subseq_dict)\n",
    "\n",
    "if WITH_HEAVY_ASSERTS:\n",
    "    from random import randint\n",
    "    seq_n, seq_dict = get_unique_substring_dict((randint(0, 1) for _ in range(1 << 20)), max_len=64)\n",
    "    print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261e90e3-4367-4652-93fa-8d7d59785971",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "def gen_candidates(p, rng_stream_dict, max_samples, sample_size, use_int = False):\n",
    "    Zpp = Zmod(p)\n",
    "    max_samples = int(max_samples)\n",
    "\n",
    "    if use_int:\n",
    "        rng_int_dict = subint_dict(rng_stream_dict)\n",
    "    \n",
    "    yield from (\n",
    "        Zpp(cand_idx - offset)\n",
    "        for cand_idx in tqdm(islice(gen_rand_offsets(p), max_samples), total=max_samples)\n",
    "        if (offset := (\n",
    "            rng_int_dict.get(gen_rng_int(p, cand_idx, sample_size))\n",
    "            if use_int else\n",
    "            rng_stream_dict.get(tuple(gen_rng_stream(p, cand_idx, sample_size)))\n",
    "        )) is not None\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec155f87-de60-4167-8d70-12b4c8c53414",
   "metadata": {},
   "source": [
    "## Server interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b2bee0-c73a-452a-aafb-e28930434d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_chal_rng_stream(\n",
    "    con,\n",
    "    msg: str,\n",
    "):\n",
    "    con.recvuntil(b\"> \")\n",
    "    con.sendline(b\"1\")\n",
    "\n",
    "    msg_g = int.from_bytes(hashlib.sha512(msg.encode()).digest(), \"big\")\n",
    "    con.recvuntil(b\"message: \")\n",
    "    con.sendline(msg.encode())\n",
    "    sig = json.loads(con.recvline().decode())['signature']\n",
    "\n",
    "    sig_key = discrete_log(ZP_FROST(sig), ZP_FROST(msg_g))\n",
    "\n",
    "    key_bits = ZZ(sig_key).digits(2, padto=QUERY_SIZE)[::-1]\n",
    "    return key_bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789cc85f-726f-4a12-b8db-30cdc01cfd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_guess(\n",
    "    con,\n",
    "    guess: str,\n",
    "):\n",
    "    con.recvuntil(b\"> \")\n",
    "    con.sendline(b\"2\")\n",
    "    con.recvuntil(b\"Reveal my snow-otp (in bits): \")\n",
    "    con.sendline(guess.encode())\n",
    "    if (flag := json.loads(con.recvline())['starshard']) == FALSE_FLAG:\n",
    "        return None\n",
    "    return flag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5757fa-4cb6-4609-8588-064d2b3ffbb9",
   "metadata": {},
   "source": [
    "## Challenge Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1707b666-f78c-420b-89a3-95e4d6e36184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pwn\n",
    "from itertools import islice\n",
    "\n",
    "from typing import Callable, TypeAlias\n",
    "\n",
    "RngSolver: TypeAlias = Callable[[list[int], int], int | None]\n",
    "\n",
    "def solve_rng(\n",
    "    rng_stream,\n",
    "    p,\n",
    "    /,\n",
    "    max_sample_size = 1 << 10,\n",
    "    max_samples = 1 << 20,\n",
    "    use_int_dict = False,\n",
    ") -> int | None:\n",
    "    rng_stream_len = len(rng_stream)\n",
    "\n",
    "    with pwn.log.progress(\"Building rng_stream window dictionary\") as plog:\n",
    "        sample_size, rng_stream_dict = get_unique_substring_dict(rng_stream, max_len=max_sample_size)\n",
    "\n",
    "        if sample_size is None:\n",
    "            plog.failure(f\"key stream dict could not be built - loosen the {max_sample_size=}\")\n",
    "            return None\n",
    "        plog.status(f\"{sample_size=}\")\n",
    "\n",
    "    with pwn.log.progress(\"Offset candidate trials\") as plog:\n",
    "        for cand_seed in gen_candidates(p, rng_stream_dict, max_samples, sample_size, use_int=use_int_dict):\n",
    "            plog.status(f\"Candidate found! {cand_seed=}\")\n",
    "            if any(\n",
    "                exp != obs\n",
    "                for exp, obs\n",
    "                in zip(rng_stream, gen_rng_stream(p, cand_seed, rng_stream_len))\n",
    "            ):\n",
    "                plog.status(\"Candidate was a mismatch - continuing...\")\n",
    "                continue\n",
    "            \n",
    "            plog.success(f\"Candidate is a match! {cand_seed=}\")\n",
    "            return cand_seed\n",
    "        else:\n",
    "            plog.failure(\"We could not find a solution! increase max_samples, or collect more bits\")\n",
    "            return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bde28a9-5aad-42bc-9824-1e1406b472dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_chal(\n",
    "    queries = LIMIT - 1, # 800, # 50, # \n",
    "    query_msg = 'const_msg',\n",
    "\n",
    "    solver_func: RngSolver = solve_rng,\n",
    "    # max_sample_size = 128, # 48, # 24, # 32, # 100, # \n",
    "    # max_samples = 5_000_000, # 400_000, # 5000, # \n",
    "\n",
    "    run_local_debug=False,\n",
    "    ip=None, \n",
    "    port=None,\n",
    "):\n",
    "    if not run_local_debug:\n",
    "        assert ip is not None\n",
    "        assert port is not None\n",
    "\n",
    "    with (\n",
    "        pwn.remote(ip, port)\n",
    "        if not run_local_debug\n",
    "        else pwn.process([\"sage\", \"-python\", TEST_SERVER_PATH])\n",
    "    ) as con:\n",
    "        \n",
    "        with pwn.log.progress(\"Setting up challenge\") as plog:\n",
    "            con.recvuntil(b\"frostrng.holly_prime = \")\n",
    "            p = ZZ(con.recvline().decode().strip())\n",
    "            Zpp = Zmod(p)\n",
    "            plog.status(f\"{p=}\")\n",
    "            if run_local_debug:\n",
    "                g = con.recvline().decode().strip(\"frostrng.sleigh_seed = \").rstrip()\n",
    "                plog.status(f\"{g=}\")\n",
    "\n",
    "        with pwn.log.progress(\"Gathering rng_stream bits\") as plog:\n",
    "            rng_stream = [\n",
    "                bit\n",
    "                for _ in tqdm(range(queries))\n",
    "                for bit in extract_chal_rng_stream(con, query_msg)\n",
    "            ]\n",
    "            assert (rng_stream_len := (queries * QUERY_SIZE)) == len(rng_stream)\n",
    "            plog.status(f\"rng_size: {rng_stream_len:_}\")\n",
    "\n",
    "        with pwn.log.progress(\"locally breaking legendre RNG seed\") as plog:\n",
    "            if (sol_seed := solver_func(\n",
    "                rng_stream,\n",
    "                p,\n",
    "                # max_sample_size=max_sample_size,\n",
    "                # max_samples=max_samples,\n",
    "                # use_int_dict=use_int_dict,\n",
    "            )) is None:\n",
    "                return p, rng_stream, None\n",
    "            sol_offset = sol_seed + rng_stream_len\n",
    "        \n",
    "        with pwn.log.progress(\"Attempting cadidate as solution\") as plog:\n",
    "            sol_key = \"\".join(str(b) for b in gen_rng_stream(p, sol_offset, KEY_SUBMISSION_SIZE))\n",
    "            plog.status(f\"{sol_key=}\")\n",
    "            if (flag := submit_guess(con, sol_key)) is None:\n",
    "                plog.failure(f\"Flag was the false flag üò±! {FALSE_FLAG=}\")\n",
    "                return p, rng_stream, sol_key\n",
    "\n",
    "            plog.success(f\"{flag=}\")\n",
    "            return flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaf8fb0-5667-46f6-960a-f2985057cf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwn.log.setLevel(\"debug\")\n",
    "\n",
    "if WITH_HEAVY_ASSERTS:\n",
    "    res = solve_chal(\n",
    "        queries=200, # 1_000, # 10_000, # 800,\n",
    "        # use_int_dict=True,\n",
    "\n",
    "        run_local_debug=True,\n",
    "        # ip=\"154.57.164.66\", \n",
    "        # port=31866,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3cc63a-1db7-4f6a-979f-134cd7b29900",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_run():\n",
    "    rng_bits, p_bits = (\n",
    "        10, 20\n",
    "        # 15, 30\n",
    "        # 20, 40\n",
    "        # 25, 50 \n",
    "        # 10, 40\n",
    "    )\n",
    "\n",
    "    p = random_prime(1 << p_bits)\n",
    "    res = solve_rng(\n",
    "        (rng_stream := tuple(tqdm(gen_rng_stream(p, (k_secret := randint(1, p-1)), (rng_size := 1 << rng_bits)), total=int(rng_size)))),\n",
    "        p,\n",
    "        max_samples = 1 << 10\n",
    "    )\n",
    "    \n",
    "    display(\n",
    "        legendre_symbol(p-1, p),\n",
    "        k_secret\n",
    "    )\n",
    "\n",
    "    Zpp = GF(p)\n",
    "    size = 1 << 10\n",
    "    a = -23\n",
    "    shift = 19\n",
    "    flip = (1 - legendre_symbol(Zpp(a), p)) // 2\n",
    "    block_size = ceil((size - shift) / abs(a))\n",
    "    \n",
    "    display(table([\n",
    "        obs := tuple(flip ^^ i for i in rng_stream[:size][shift::abs(a)][::(-1 if a < 0 else 1)]),\n",
    "        exp := tuple(gen_rng_stream(p, Zpp(k_secret + shift) / Zpp(a), block_size, backwards=(a < 0)))[::(-1 if a < 0 else 1)],\n",
    "        exp_2 := tuple(gen_rng_stream(p, idx := ((Zpp(k_secret + shift) / Zpp(a)) - (block_size - 1 if a < 0 else 0)), block_size)),\n",
    "    ]))\n",
    "    return obs == exp == exp_2\n",
    "\n",
    "if WITH_HEAVY_ASSERTS:\n",
    "    assert test_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bfeeb5-0047-4ed5-9751-2b227db36336",
   "metadata": {},
   "source": [
    "# Extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfa9199-d249-490a-bc57-3742b2bdb80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WITH_HEAVY_ASSERTS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9335f06a-c530-4a81-b024-1450e2d67aa8",
   "metadata": {},
   "source": [
    "## Better Subsequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1778d41c-16c4-439e-9bc7-7988ca3bc03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sage.data_structures.bounded_integer_sequences import BoundedIntegerSequence\n",
    "from typing import Generator, Sequence, Iterable\n",
    "\n",
    "def parse_stream_to_bseq(it: Iterable) -> BoundedIntegerSequence:\n",
    "    return BoundedIntegerSequence(2, list(it))\n",
    "\n",
    "def sliding_window_seq[T: Sequence](seq: T, n: int) -> Generator[T, None, None]:\n",
    "    for i in range(len(seq) + 1 - n):\n",
    "        yield seq[i:i+n]\n",
    "\n",
    "display(\n",
    "    list(sliding_window_seq(_bseq := parse_stream_to_bseq([1,0,1,1,0,1]), 5)),\n",
    "    _bseq.bound(),\n",
    ")\n",
    "del _bseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a3a53d-59e2-4e6b-97f9-c147385465ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from typing import Generator, Sequence, Iterable, TypeAlias\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "\n",
    "ArrSeq: TypeAlias = NDArray[np.uint8]\n",
    "\n",
    "DEFAULT_BIT_ORDER = 'big'\n",
    "\n",
    "def parse_stream_to_arr(it: Iterable, size: int | None = None) -> ArrSeq:\n",
    "    if size is None:\n",
    "        if not isinstance(it, Sequence):\n",
    "            it = tuple(it)\n",
    "        size = len(it)\n",
    "\n",
    "    it = iter(it)\n",
    "    data = np.fromiter(it, dtype=np.uint8, count=size)\n",
    "    try:\n",
    "        next(it)\n",
    "    except StopIteration:\n",
    "        pass\n",
    "    return data\n",
    "\n",
    "def split_column_arr[T](arr: NDArray[T], step: int) -> Generator[NDArray[T], None, None]:\n",
    "    size = len(arr)\n",
    "    overflow = ((-size) % step)\n",
    "    matrix = np.pad(arr, (0, overflow), mode='empty').reshape(step, -1, order='F').copy(order=\"C\") #  ,mode='constant', constant_values=255\n",
    "    for idx, col in enumerate(matrix):\n",
    "        if idx >= (step - overflow):\n",
    "            col = col[:-1]\n",
    "        yield idx, col\n",
    "\n",
    "def sliding_window_arr[T](arr: NDArray[T], win_size: int) -> Generator[NDArray[T], None, None]:\n",
    "    if len(arr) < win_size: return\n",
    "    yield from np.lib.stride_tricks.sliding_window_view(arr, win_size)\n",
    "\n",
    "def neg_arr(arr: ArrSeq) -> ArrSeq:\n",
    "    return np.bitwise_xor(arr, 1)\n",
    "\n",
    "def arr_to_int(arr: ArrSeq) -> int:\n",
    "    return int.from_bytes(np.packbits(arr, bitorder=DEFAULT_BIT_ORDER).tobytes(), DEFAULT_BIT_ORDER)\n",
    "\n",
    "def int_to_arr(i: int, size: int) -> ArrSeq:\n",
    "    return np.unpackbits(np.frombuffer(i.to_bytes(size, DEFAULT_BIT_ORDER), dtype=np.uint8), bitorder=DEFAULT_BIT_ORDER)\n",
    "\n",
    "def gen_rng_arr(p, offset, size, backwards=False):\n",
    "    return parse_stream_to_arr(gen_rng_stream(p, offset, size, backwards=False), size)\n",
    "\n",
    "\n",
    "display(\n",
    "    size := 133,\n",
    "    stride := 7,\n",
    "    table([[\n",
    "        k,\n",
    "        v,\n",
    "        v_bits := np.unpackbits(v),\n",
    "        h := arr_to_int(v_bits),\n",
    "        int_to_arr(h, ceil(size / stride))\n",
    "    ] for (k, v) in split_column_arr(np.arange(size, dtype=np.uint8), stride)]),\n",
    "    (4 << (8*2)) + (8 << (8*1)) + (12 << (8*0)),\n",
    "    data := gen_rng_arr(random_prime(1 << 50), 31337, 1 << 6),\n",
    "    data_int := arr_to_int(data),\n",
    "    hash(data_int),\n",
    "    neg_arr(data)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3612feda-0456-4cdc-bda1-8f94330c3fab",
   "metadata": {},
   "source": [
    "## Better Subsequences Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cd47fb-15e1-4ea3-b6c1-7824a7c79ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import IntEnum\n",
    "from dataclasses import dataclass\n",
    "from typing import Generator\n",
    "from functools import cached_property\n",
    "\n",
    "from sage.rings.finite_rings.finite_field_prime_modn import FiniteField_prime_modn\n",
    "\n",
    "k = var('k', latex_name=r'k^\\prime')\n",
    "\n",
    "@dataclass(frozen=True, slots=True)\n",
    "class StreamViewCase:\n",
    "    is_neg: bool\n",
    "    scale: int\n",
    "    shift: int\n",
    "    offset: int\n",
    "\n",
    "    @property\n",
    "    def sign(self):\n",
    "        return (-1 if self.is_neg else 1)\n",
    "\n",
    "    def gen_from_subseq(self, seq: Sequence, field: FiniteField_prime_modn) -> Generator[int, None, None]:\n",
    "        flip = (1 - legendre_symbol(field(self.scale * self.sign), field.order())) // 2\n",
    "        yield from (flip ^^ i for i in seq[self.shift::self.scale][::self.sign][self.offset:])\n",
    "\n",
    "    def _latex_(self):\n",
    "        return (\n",
    "            r\"\\left[\\begin{array}{c|c}\"\n",
    "            r\"\\operatorname{Sign}(\\cdot) & \" + (sign := (r\"\\ominus\" if self.is_neg else r\"\\oplus\")) + r\" \\\\\"\n",
    "            r\"|\\textit{d}| & \" + latex(self.scale) + r\" \\\\\"\n",
    "            r\"\\textit{i} & \" + latex(self.shift) + r\" \\\\\"\n",
    "            r\"\\varepsilon_\\textit{L} & \" + latex(self.offset) + r\" \\\\\" \n",
    "            r\"\\end{array}\\right]\"\n",
    "        )\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class StreamView:\n",
    "\n",
    "    case: StreamViewCase\n",
    "    M: int\n",
    "    Fp: FiniteField_prime_modn\n",
    "\n",
    "    @cached_property\n",
    "    def L(self):\n",
    "        return ceil((self.M - self.case.shift) / self.case.scale)\n",
    "\n",
    "    @cached_property\n",
    "    def view_suffix_size(self):\n",
    "        return int(self.L - self.case.offset)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.view_suffix_size\n",
    "\n",
    "    @cached_property\n",
    "    def k(self):\n",
    "        return (\n",
    "            ((k + self.case.shift) / (self.case.sign * self.case.scale))\n",
    "            + self.case.offset\n",
    "            - (self.L - 1 if self.case.is_neg else 0)\n",
    "        )\n",
    "\n",
    "    @cached_property\n",
    "    def k_fp(self):\n",
    "        return self.k.polynomial(base_ring=self.Fp)\n",
    "\n",
    "    def gen_from_subseq(self, seq: Sequence) -> Generator[int, None, None]:\n",
    "        return self.case.gen_from_subseq(seq, self.Fp)\n",
    "\n",
    "    def gen_from_k(self, k_seed: int, size: int):\n",
    "        return gen_rng_stream(self.Fp.order(), self.k_fp.subs(k_seed), size)\n",
    "\n",
    "    def _latex_(self):\n",
    "        return (\n",
    "            r\"\\left[\\begin{array}{c|c}\"\n",
    "            r\"\\text{CTX} & \" + latex(self.case) + r\" \\\\\"\n",
    "            r\"\\hline\"\n",
    "            r\"\\|\\textit{M}\\| & \" + latex(self.M) + r\" \\\\\"\n",
    "            r\"\\|\\textit{L} + \\varepsilon\\| & \" + latex(self.L) + r\" \\\\\"\n",
    "            r\"\\hline\"\n",
    "            r\"\\{ \\cdot \\}_\\textit{L} & \\left\\{\" + (\n",
    "                (r\"- \\left( \" if self.case.is_neg else \"\")\n",
    "                + r\"\\frac{\" + latex(k + self.case.shift) + r\"}{\" + latex(self.case.scale) + r\"}\"\n",
    "                + (\" + \" + latex(\n",
    "                        self.view_suffix_size - 1\n",
    "                        if self.case.is_neg\n",
    "                        else self.case.offset\n",
    "                    ) if self.case.offset else \"\"\n",
    "                ) + (r\" \\right)\" if self.case.is_neg else \"\")\n",
    "            ) + r\"\\right\\}_{\" + latex(self.view_suffix_size)  + r\"} \\\\\"\n",
    "            r\"\\textit{k}^\\prime \\in \\mathbb{Q} & \" + latex(self.k) + r\" \\\\\"\n",
    "            r\"\\hline\"\n",
    "            r\"\\mathbf{F}_p & \" + latex(self.Fp) + r\" \\\\\"\n",
    "            r\"\\textit{k} \\in \\mathbf{F}_p & \" + latex(self.k_fp) + r\" \\\\\"\n",
    "            r\"\\end{array}\\right]\"\n",
    "        )\n",
    "\n",
    "display(\n",
    "    _case := StreamViewCase(\n",
    "        scale=9,\n",
    "        is_neg=True,\n",
    "        shift=7,\n",
    "        offset=68,\n",
    "    ),\n",
    "    _view := StreamView(\n",
    "        case=_case,\n",
    "        M=1024,\n",
    "        Fp=GF(582041651),\n",
    "    ),\n",
    "    len(_view)\n",
    ")\n",
    "del _case, _view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adadc61-7187-4b19-aaaa-03f6033cd885",
   "metadata": {},
   "source": [
    "## Better generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73751c3-b879-4a7d-b369-0ae42b219245",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain, islice\n",
    "from tqdm.auto import tqdm\n",
    "from functools import cached_property\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import TypeAlias\n",
    "\n",
    "Sample: TypeAlias = tuple[BoundedIntegerSequence, StreamViewCase]\n",
    "\n",
    "@dataclass(frozen=True, slots=True)\n",
    "class LegendreScalars:\n",
    "    scale: int\n",
    "    is_flip: bool\n",
    "    jump: int\n",
    "\n",
    "    def __post_init__(self):\n",
    "        object.__setattr__(self, \"scale\", int(self.scale))\n",
    "        object.__setattr__(self, \"jump\", int(self.jump))\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class LegendreSampler:\n",
    "    p: int\n",
    "    L: int\n",
    "    gen_len: int = int(1 << 20) # int(500_000)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        object.__setattr__(self, \"p\", int(self.p))\n",
    "        object.__setattr__(self, \"L\", int(self.L))\n",
    "        object.__setattr__(self, \"gen_len\", int(self.gen_len))\n",
    "\n",
    "    @cached_property\n",
    "    def Fp(self):\n",
    "        return GF(self.p)\n",
    "\n",
    "    @cached_property\n",
    "    def scalar_symbols(self):\n",
    "        return tuple(\n",
    "            LegendreScalars(\n",
    "                scale=scale,\n",
    "                is_flip=(legendre_symbol(scale, self.p) == -1),\n",
    "                jump=scale * self.L\n",
    "            ) # self.Fp(scale)\n",
    "            for scale in chain([1], primes(2, ceil(self.gen_len / self.L)))\n",
    "        )\n",
    "\n",
    "    @cached_property\n",
    "    def idx_size(self):\n",
    "        return int(sum(scalar.scale for scalar in self.scalar_symbols))\n",
    "\n",
    "    @cached_property\n",
    "    def bit_size(self):\n",
    "        return self.idx_size * self.L\n",
    "\n",
    "    @cached_property\n",
    "    def gain(self):\n",
    "        return self.bit_size / self.gen_len\n",
    "\n",
    "    def sample_from_bseq(\n",
    "        self,\n",
    "        bseq: BoundedIntegerSequence,\n",
    "        *,\n",
    "        show_tqdm: int = int(0),\n",
    "    ) -> Generator[Sample, None, None]:\n",
    "        assert len(bseq) == self.gen_len, f\"Mis-sized bseq passed! {self.gen_len=}, {len(bseq)=}, {bseq=}\"\n",
    "        \n",
    "        ## Even though all the different scales of `q` have their own unique\n",
    "        ## `legendre_symbol(q, p)` values, since q != 0, it must resolve to\n",
    "        ## =>  1, i.e. the sequence values remains unchanged\n",
    "        ## => -1, i.e. the sequence values must further be negated\n",
    "        ## ergo, `neg_seq` plays the same role as `seq`\n",
    "        ## just for cases where `legendre_symbol(q, p) == -1`\n",
    "        neg_bseq = parse_stream_to_bseq(1 - bit for bit in bseq)\n",
    "\n",
    "        yield from tqdm((\n",
    "            (\n",
    "                cur_seq[shift:scalar.jump:scalar.scale],\n",
    "                StreamViewCase(\n",
    "                    scale=scalar.scale,\n",
    "                    is_neg=False,\n",
    "                    shift=shift,\n",
    "                    offset=0,\n",
    "            ))\n",
    "            for scalar in tqdm(\n",
    "                self.scalar_symbols,\n",
    "                desc=\"Subsequence scalars\",\n",
    "                position=int(1),\n",
    "                leave=False,\n",
    "                disable=(show_tqdm < 1)\n",
    "            )\n",
    "            if (cur_seq := (neg_bseq if scalar.is_flip else bseq)) is not None\n",
    "            for shift in tqdm(\n",
    "                range(scalar.scale),\n",
    "                desc=\"Subsequence sub-scalar shift\",\n",
    "                position=int(2),\n",
    "                leave=False,\n",
    "                disable=(show_tqdm < 2)\n",
    "            )\n",
    "        ),\n",
    "            desc=f\"Yielding L-size sample subsequences ({self.L=})\",\n",
    "            total=self.idx_size,\n",
    "            position=int(0),\n",
    "            leave=False,\n",
    "            disable=(show_tqdm < 0),\n",
    "        )\n",
    "        \n",
    "\n",
    "    def sample_from_idx(\n",
    "        self,\n",
    "        idx: int,\n",
    "        *,\n",
    "        show_tqdm: int = int(0),\n",
    "    ) -> Generator[Sample, None, None]:\n",
    "        return self.sample_from_bseq(\n",
    "            parse_stream_to_bseq(tqdm(\n",
    "                gen_rng_stream(self.p, idx, self.gen_len),\n",
    "                desc=f\"Generating source sequence for {idx=}\",\n",
    "                total=int(self.gen_len),\n",
    "                position=int(0),\n",
    "                leave=False,\n",
    "                disable=(show_tqdm < 0)\n",
    "            )),\n",
    "            show_tqdm=show_tqdm,\n",
    "        )\n",
    "\n",
    "    def gen_samples(\n",
    "        self,\n",
    "        max_idx_samples: int | None = None,\n",
    "        *,\n",
    "        show_tqdm: int = int(0),\n",
    "    ) -> Generator[tuple[int, Sample], None, None]:\n",
    "        if max_idx_samples is not None:\n",
    "            max_idx_samples = int(max_idx_samples)\n",
    "\n",
    "        yield from tqdm((\n",
    "                (idx, sample)\n",
    "                for idx in tqdm(\n",
    "                    islice(gen_rand_offsets(self.p), max_idx_samples),\n",
    "                    desc=\"Source sequence at randomized idx\",\n",
    "                    position=int(1),\n",
    "                    total=max_idx_samples,\n",
    "                    leave=False,\n",
    "                    disable=(show_tqdm < -1 or max_idx_samples is None),\n",
    "                )\n",
    "                for sample in self.sample_from_idx(idx, show_tqdm=show_tqdm)\n",
    "            ),\n",
    "            desc=\"Yielding samples\",\n",
    "            position=int(0),\n",
    "            total=(\n",
    "                self.idx_size * max_idx_samples\n",
    "                if max_idx_samples is not None\n",
    "                else None\n",
    "            ),\n",
    "            leave=True,\n",
    "            disable=(show_tqdm < -2),   \n",
    "        )\n",
    "\n",
    "    def wrap_case(self, case: StreamViewCase) -> StreamView:\n",
    "        return StreamView(\n",
    "            case=case,\n",
    "            M=self.gen_len,\n",
    "            Fp=self.Fp\n",
    "        )\n",
    "\n",
    "    def _latex_(self):\n",
    "        return (\n",
    "            r\"\\left \\{ \\frac{ \\cdot } { \"\n",
    "            + latex(self.p)\n",
    "            + r\"} \\right \\}_{\"\n",
    "            + latex(self.gen_len)\n",
    "            + r\"} \\mapsto \"\n",
    "            + r\"\\left \\{ \"\n",
    "            + r\"\\left \\{ \\frac{ \\cdot } { \"\n",
    "            + latex(self.p)\n",
    "            + r\"} \\right \\}_{\"\n",
    "            + latex(self.L)\n",
    "            + r\"}\"\n",
    "            + r\"\\right \\}_{\"\n",
    "            + latex(self.idx_size)\n",
    "            + r\"} (\\approx \"\n",
    "            + self.gain\n",
    "            + r\")\"\n",
    "        )\n",
    "\n",
    "display(\n",
    "    LegendreSampler(p=582041651, L=32),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f5117c-e97d-420c-8890-4fa468dc52e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampler = LegendreSampler(p=p, L=97)\n",
    "# all(sampler.sample_from_idx(1000))\n",
    "# all(sampler.gen_samples(1))\n",
    "# all(hash(s) for _, s, _ in LegendreSampler(p, 50, gen_len=100_000).gen_samples(10)) # 500_000 # 1_000_000\n",
    "\n",
    "def test_sampler():\n",
    "    sampler = LegendreSampler(p=random_prime(1 << 39, 1 << 40), L=97, gen_len=100_000)\n",
    "    cases = {}\n",
    "    for idx, (subseq_exp, case) in sampler.gen_samples(1):\n",
    "        view = sampler.wrap_case(case)\n",
    "        if subseq_exp != (subseq_obs := parse_stream_to_bseq(view.gen_from_k(idx, sampler.L))):\n",
    "            display(\n",
    "                table([subseq_exp, subseq_obs]),\n",
    "                idx,\n",
    "                view,\n",
    "            )\n",
    "            raise RuntimeError(\"Self-contradicting result found!\")\n",
    "    \n",
    "        if (case := (idx, subseq_exp)) in cases:\n",
    "            print(\"Overlap found!\")\n",
    "            if idx != (view.k_fp - cases[case].k_fp).any_root():\n",
    "                raise RuntimeError(\"Repeated view found!\")\n",
    "    \n",
    "        cases[case] = view\n",
    "    return True\n",
    "    \n",
    "\n",
    "if WITH_HEAVY_ASSERTS:\n",
    "    assert test_sampler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7145163a-dce9-4a64-9eec-54e9e00414e5",
   "metadata": {},
   "source": [
    "## Better Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae07652-5171-4dd2-a196-bfc950f081c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <https://docs.python.org/3/library/collections.abc.html#collections-abstract-base-classes>\n",
    "from collections.abc import Mapping\n",
    "from typing import Self\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from itertools import islice\n",
    "\n",
    "from rbloom import Bloom\n",
    "from pyfusefilter import Fuse16\n",
    "\n",
    "DEFAULT_MAX_L = 256\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class LegendreRngDict(Mapping):\n",
    "    p: int\n",
    "    L: int\n",
    "    source: BoundedIntegerSequence\n",
    "    table: dict[BoundedIntegerSequence, StreamViewCase]\n",
    "    bloom_prob: int = 0.01\n",
    "\n",
    "    @cached_property\n",
    "    def M(self):\n",
    "        return len(self.source)\n",
    "\n",
    "    @cached_property\n",
    "    def Fp(self):\n",
    "        return GF(self.p)\n",
    "\n",
    "    def wrap_case(self, case: StreamViewCase) -> StreamView:\n",
    "        return StreamView(\n",
    "            case=case,\n",
    "            M=self.M,\n",
    "            Fp=self.Fp\n",
    "        )\n",
    "\n",
    "    # TODO: replace this with a Fuse16 variant that doesn't have a bad hash function\n",
    "    # Also, experiement with rBloom first to see if this is even useful or not\n",
    "    @cached_property\n",
    "    def bloom_filter(self) -> Bloom:\n",
    "        print(\"Building Bloom Filter\")\n",
    "        prob_filter = Bloom(len(self), self.bloom_prob)\n",
    "        prob_filter.update(self.table.keys())\n",
    "        print(f\"Bloom Filter built! {prob_filter=}\")\n",
    "        return prob_filter\n",
    "\n",
    "    # TODO: replace this with a Fuse16 variant that doesn't have a bad hash function\n",
    "    # Also, experiement with rBloom first to see if this is even useful or not\n",
    "    @cached_property\n",
    "    def fuse_filter(self) -> Fuse16:\n",
    "        print(\"Building Fuse16 Filter\")\n",
    "        prob_filter = Fuse16(len(self))\n",
    "        prob_filter.populate(self.table.keys())\n",
    "        # prob_filter = Bloom(len(self), 0.01)\n",
    "        # prob_filter.update(self.table.keys())\n",
    "        print(f\"Fuse16 Filter built! {prob_filter=}\")\n",
    "        return prob_filter\n",
    "\n",
    "    @classmethod\n",
    "    def expect_scalars(cls, m, l, as_int=True):\n",
    "        exp = (m - 1) / (l - 1)\n",
    "        if as_int:\n",
    "            return int(floor(exp))\n",
    "        return exp\n",
    "\n",
    "    @classmethod\n",
    "    def expect_size(cls, m, l, parts=False, as_int=True):\n",
    "        \"\"\"given a continuous rng_stream sequence and the subsequence length, return the expected number of derivable substrings\n",
    "\n",
    "        Source: <https://eprint.iacr.org/2020/098.pdf#page=7>\n",
    "        \"\"\"\n",
    "        l_d = cls.expect_scalars(m, l, as_int=as_int)\n",
    "        total = 2*m*l_d\n",
    "        exceptions = (l-1)*l_d*(l_d+1)\n",
    "\n",
    "        if as_int:\n",
    "            exceptions = int(exceptions)\n",
    "            total = int(total)\n",
    "        if parts:\n",
    "            return exceptions, total\n",
    "        return total - exceptions        \n",
    "\n",
    "    @classmethod\n",
    "    def min_L(cls, m, max_L=DEFAULT_MAX_L, strict=True) -> int:\n",
    "        \"\"\"Calculates the minimum L size based on the pidgenhole principle\n",
    "\n",
    "        If strict, gives the direct bound (i.e. against N := 2^L spaces)\n",
    "        If loose, gives the bound factoring in the birthday paradox\n",
    "            (i.e. against (N := 2^L)^2 = 2^2L spaces - since to avoid collisions, you need ~N^2 space)\n",
    "        \"\"\"\n",
    "        M, L = var('M, L')\n",
    "        lhs = cls.expect_size(M, L, as_int=False)\n",
    "        expr = (lhs - (2**(L if strict else L/2))).expand() # \n",
    "        return int(ceil(find_root(expr.subs(M=m) == 0, 1, max_L)))\n",
    "\n",
    "    @classmethod\n",
    "    def gen_seq_subviews(cls, bseq: BoundedIntegerSequence, p: int, L: int, no_tqdm=False) -> Generator[Sample, None, None]:\n",
    "        Fp = GF(p)\n",
    "        size = len(bseq)\n",
    "\n",
    "        neg_bseq = parse_stream_to_bseq(1 - bit for bit in bseq)\n",
    "        is_neg_flip = (legendre_symbol(Fp(-1), p) == -1)\n",
    "\n",
    "        for scale in tqdm(\n",
    "            range(1, 1 + cls.expect_scalars(size, L, as_int=True)),\n",
    "            position=0,\n",
    "            desc=\"Substring scalars\",\n",
    "            disable=no_tqdm\n",
    "        ):\n",
    "            is_scale_flip = (legendre_symbol(scale, p) == -1)\n",
    "            \n",
    "            for is_neg, sign in tqdm(\n",
    "                ((False, int(1)), (True, int(-1))),\n",
    "                position=1,\n",
    "                leave=False,\n",
    "                desc=\"Substring reflection\",\n",
    "                disable=no_tqdm\n",
    "            ):\n",
    "                cur_seq = (neg_bseq if (is_scale_flip != (is_neg_flip and is_neg)) else bseq)\n",
    "                yield from ((\n",
    "                    subseq,\n",
    "                    StreamViewCase(\n",
    "                        scale=scale,\n",
    "                        is_neg=is_neg,\n",
    "                        shift=shift,\n",
    "                        offset=i,\n",
    "                        # M=size,\n",
    "                        # Fp=Fp,\n",
    "                    ))\n",
    "                    for shift in tqdm(range(scale), position=2, leave=False, desc=\"Substring sub-scalar shift\", disable=no_tqdm)\n",
    "                    for i, subseq in enumerate(sliding_window_seq(cur_seq[shift::scale][::sign], L))\n",
    "                )\n",
    "\n",
    "    # TODO: replace the messy verbosity system with a proper logging system\n",
    "    @classmethod\n",
    "    def build_subview_dict(\n",
    "        cls,\n",
    "        it,\n",
    "        p,\n",
    "        *,\n",
    "        max_L=DEFAULT_MAX_L,\n",
    "        strict_L=False,\n",
    "        # Visuals\n",
    "        clear_bar=True,\n",
    "        verbosity=0,\n",
    "    ) -> Self | int | None:\n",
    "        Fp = GF(p)\n",
    "        seq = (parse_stream_to_bseq(it) if not isinstance(it, BoundedIntegerSequence) else it)\n",
    "        m = len(seq)\n",
    "        min_len = cls.min_L(m, strict=strict_L)\n",
    "        \n",
    "        if verbosity >= 0: display(f\"{Fp}, {m=}, {min_len=}\")\n",
    "    \n",
    "        l_bound = 0\n",
    "        prev_overlap_seeds = set()\n",
    "        for l in (outer_tqdm := tqdm(\n",
    "            range(min_len, max_L + 1),\n",
    "            desc=\"Trialling substring lengths for unique index dictionary\",\n",
    "            initial=int(min_len),\n",
    "            total=int(max_L),\n",
    "            maxinterval=5,\n",
    "            disable=(verbosity < -2),\n",
    "        )):\n",
    "            if l_bound >= l:\n",
    "                if verbosity >= 1: display(f\"Skipped iteration {l=}\")\n",
    "                continue\n",
    "    \n",
    "            subseq_total = cls.expect_size(m, l)\n",
    "            subseq_dict = {}\n",
    "            for subseq, case in (inner_tqdm := tqdm(\n",
    "                cls.gen_seq_subviews(\n",
    "                    bseq=seq,\n",
    "                    p=p,\n",
    "                    L=l,\n",
    "                    no_tqdm=True\n",
    "                ),\n",
    "                desc=f\"Trial - |L|={l:#02x}\",\n",
    "                total=subseq_total,\n",
    "                disable=(verbosity < -1),\n",
    "            )):\n",
    "                if ((prev_case := subseq_dict.setdefault(subseq, case)) is case):\n",
    "                    # subseq_dict[subseq] = case\n",
    "                    continue\n",
    "\n",
    "                # Constradiction was found, and inner loop will be exited - I am doing deletions\n",
    "                # pre-emptively and explicitly just to absolutely make sure I have as little wasted memory\n",
    "                del subseq_dict\n",
    "\n",
    "                prev_view = StreamView(\n",
    "                    case=prev_case,\n",
    "                    M=m,\n",
    "                    Fp=Fp,\n",
    "                )\n",
    "                cur_view = StreamView(\n",
    "                    case=case,\n",
    "                    M=m,\n",
    "                    Fp=Fp,\n",
    "                )\n",
    "    \n",
    "                if clear_bar and not inner_tqdm.disable: tqdm_remover(inner_tqdm)\n",
    "                if verbosity >= 2: display(table([[subseq, prev_view, cur_view]]))\n",
    "                \n",
    "                if 1 != (\n",
    "                    overlap := (prev_view.k_fp - cur_view.k_fp)\n",
    "                ).degree():\n",
    "                    if verbosity >= 1: display(f\"Local overlap found - but {l=} is broken, with {overlap=}...\")\n",
    "                    break\n",
    "                    \n",
    "                overlap_seed = overlap.any_root()\n",
    "                if verbosity >= 1: display(f\"Local overlap found - {l=}, {overlap_seed=}\")\n",
    "\n",
    "                if (\n",
    "                    overlap_seed not in prev_overlap_seeds\n",
    "                    and all(\n",
    "                        exp == obs\n",
    "                        for exp, obs\n",
    "                        in zip(seq, gen_rng_stream(p, overlap_seed, len(seq)))\n",
    "                )):\n",
    "                    if verbosity >= 0: display(f\"SEED FOUND! {overlap_seed=}\")\n",
    "                    return int(overlap_seed)\n",
    "    \n",
    "                prev_overlap_seeds |= {overlap_seed}\n",
    "                if verbosity >= 0: display(f\"False seed found - {l:#02x} / {overlap_seed=}\")\n",
    "\n",
    "                if (\n",
    "                    (mismatch_idx := next((\n",
    "                        idx\n",
    "                        for idx, (b0, b1)\n",
    "                        in enumerate(zip(prev_view.gen_from_subseq(seq), cur_view.gen_from_subseq(seq)))\n",
    "                        if b0 != b1\n",
    "                    ), None)) is not None\n",
    "                    and (skipper := mismatch_idx - l) > 0\n",
    "                ):\n",
    "                    if verbosity >= 0: display(f\"Overlap size yields L-jump {l:#02x} ‚Ü¶ {mismatch_idx+1:#02x} - skipping {skipper} intermediate iterations!\")\n",
    "                    l_bound = mismatch_idx\n",
    "\n",
    "                break\n",
    "            else:\n",
    "                if verbosity >= 0: display(f\"View dictionary successfully built!\")\n",
    "                return cls(\n",
    "                    p=p,\n",
    "                    L=l,\n",
    "                    source=seq,\n",
    "                    table=subseq_dict,\n",
    "                )\n",
    "        return None\n",
    "    \n",
    "    def assert_from_source(self):\n",
    "        assert len(self) == (\n",
    "            len_exp := self.expect_size(self.M, self.L)\n",
    "        ), f\"LegendreRngDict Source Size Failure - {len_exp=}, {len(self)=}\"\n",
    "        \n",
    "        for k_exp, v in tqdm(self.items(), desc=\"Running source validation asserts\"):\n",
    "            assert k_exp == (\n",
    "                k_obs := parse_stream_to_bseq(islice(self.wrap_case(v).gen_from_subseq(self.source), self.L))\n",
    "            ), f\"LegendreRngDict Source Validation Failure - {k_exp=}, {k_obs=}, {v=}\"\n",
    "    \n",
    "    def assert_from_key(self, k_val):\n",
    "        for k_exp, v in tqdm(self.items(), desc=\"Running key validation asserts\"):\n",
    "            assert k_exp == (\n",
    "                k_obs := parse_stream_to_bseq(self.wrap_case(v).gen_from_k(k_val, self.L))\n",
    "            ), f\"LegendreRngDict Key Validation Failure - {k_exp=}, {k_obs=}, {v=}\"\n",
    "    \n",
    "    def assert_fuse_filter(self):\n",
    "        for k_exp in tqdm(self.keys(), desc=\"Running filter validation asserts\"):\n",
    "            assert k_exp in self.fuse_filter, f\"LegendreRngDict Filter Validation Failure - {k_exp=}, {self.fuse_filter=}\"\n",
    "\n",
    "    def gen_overlaps(self, max_samples: int | None):\n",
    "        max_samples = int(max_samples)\n",
    "\n",
    "        yield from (\n",
    "            (cand_idx, self.wrap_case(case_match))\n",
    "            for cand_idx in tqdm(islice(gen_rand_offsets(p), max_samples), total=max_samples)\n",
    "            if (case_match := self.get(parse_stream_to_bseq(gen_rng_stream(p, cand_idx, self.L)))) is not None\n",
    "        )\n",
    "\n",
    "    def gen_overlaps_from_sampler(\n",
    "        self,\n",
    "        sampler: LegendreSampler,\n",
    "        max_idx_samples: int | None = None,\n",
    "        *,\n",
    "        show_tqdm: int = int(0),\n",
    "        skip_fuse = True,\n",
    "        skip_bloom = True,\n",
    "    ):\n",
    "        assert all((\n",
    "            sampler.Fp == self.Fp,\n",
    "            sampler.p == self.p,\n",
    "            sampler.L == self.L,\n",
    "        ))\n",
    "\n",
    "        yield from tqdm((\n",
    "                (sampler.wrap_case(case).k_fp(idx), self.wrap_case(case_match))\n",
    "                for idx in tqdm(\n",
    "                    islice(gen_rand_offsets(self.p), max_idx_samples),\n",
    "                    desc=\"Sampling `idx`\",\n",
    "                    position=int(1),\n",
    "                    total=max_idx_samples,\n",
    "                    leave=False,\n",
    "                    disable=(show_tqdm < -1 or max_idx_samples is None),\n",
    "                )\n",
    "                for subseq, case in sampler.sample_from_idx(idx, show_tqdm=show_tqdm)\n",
    "                if skip_fuse or (subseq in self.fuse_filter)\n",
    "                if skip_bloom or (subseq in self.bloom_filter)\n",
    "                if (case_match := self.get(subseq)) is not None\n",
    "            ),\n",
    "            desc=\"Yielding samples\",\n",
    "            position=int(0),\n",
    "            # total=(\n",
    "            #     sampler.idx_size * max_idx_samples\n",
    "            #     if max_idx_samples is not None\n",
    "            #     else None\n",
    "            # ),\n",
    "            leave=True,\n",
    "            disable=(show_tqdm < -2),   \n",
    "        )\n",
    "\n",
    "    def gen_overlaps_fast(\n",
    "        self,\n",
    "        gen_len: int | None = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        return self.gen_overlaps_from_sampler(\n",
    "            LegendreSampler(\n",
    "                p=self.p,\n",
    "                L=self.L,\n",
    "                **{ k: v\n",
    "                    for k, v\n",
    "                    in (\n",
    "                        (\"gen_len\", gen_len),\n",
    "                    ) if v is not None\n",
    "                }\n",
    "            ),\n",
    "            **kwargs\n",
    "        )\n",
    "    \n",
    "    # <https://docs.python.org/3/library/collections.abc.html#collections.abc.Mapping>\n",
    "    def __getitem__(self, x):\n",
    "        return self.table[x]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.table)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.table)\n",
    "\n",
    "    def _latex_(self):\n",
    "        return (\n",
    "            r\"\\left \\{ \\frac{ \\cdot } { \"\n",
    "            + latex(self.p)\n",
    "            + r\"} \\right \\}_{\"\n",
    "            + latex(self.L)\n",
    "            + r\"} \\xtwoheadrightarrow[\\bot]{\\text{lookup}} \" # \\textcolor{red}{}\n",
    "                # \\xhookrightarrow{\\text{lookup}}\n",
    "                # \\overset{\\text{lookup}}{\\rightharpoonup}\n",
    "                # \\xmapsto{\\text{lookup}}\n",
    "                # \\leadsto \\rightsquigarrow \\hookrightarrow\n",
    "            + r\"\\left[\" # \\{\\bot\\} \\cup # \\text{Option} \\left( \n",
    "            + r\"\\frac{\\textit{k} + \\textit{i}}{\\textit{d}} \\right]_{\"\n",
    "            + r\"\\varepsilon_\\textit{L} }^{\"\n",
    "            + latex(self.L)\n",
    "            + r\" + \\varepsilon_\\textit{L} }\" # \\right)\n",
    "        )\n",
    "\n",
    "display(\n",
    "    LegendreRngDict.build_subview_dict(\n",
    "        gen_rng_stream(\n",
    "            _p := 1112707506698441, # random_prime(1 << 50)\n",
    "            _k_secret := 1234567890, # randint(1, p-1)\n",
    "            _rng_size := 1 << 10,\n",
    "        ),\n",
    "        _p,\n",
    "        verbosity=-3,\n",
    "        strict_L=False\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff54b1a8-d04f-44cb-8bdd-8234c0ab807f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dictionary(p, k_seed):\n",
    "    rng_len = len(rng_substream := tuple(gen_rng_stream(p, k_seed, (1 << 10))))\n",
    "    \n",
    "    match LegendreRngDict.build_subview_dict(rng_substream, p, strict_L=True):\n",
    "        case None:\n",
    "            raise RuntimeError(\"Failed to build dictionary!\")\n",
    "    \n",
    "        case int(seed):\n",
    "            assert k_seed == seed, f\"BAD SEED - exp={k_seed} obs={seed}\"\n",
    "            display(f\"Seed recovered - {seed=} {rng_len=}\")\n",
    "            assert False, \"Pre-emptive solution\"\n",
    "    \n",
    "        case LegendreRngDict() as windows:\n",
    "            win_len = len(windows)\n",
    "    \n",
    "            display(f\"{rng_len=}, {win_len=}, {windows.L=}\")\n",
    "            if WITH_HEAVY_ASSERTS:\n",
    "                windows.assert_from_source()\n",
    "                windows.assert_from_key(k_seed)\n",
    "                windows.assert_fuse_filter()\n",
    "    \n",
    "            size = 50\n",
    "            display(table(win_slice := list(islice(enumerate(windows.values()), off := (win_len - size) // 2, off + size))).transpose())\n",
    "\n",
    "    return True\n",
    "\n",
    "set_random_seed(31337)\n",
    "assert test_dictionary(\n",
    "    _p := random_prime(1 << 40),\n",
    "    _k_seed := randint(1, _p-1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb3d8cd-cb0a-45c8-89bb-ac157d85fa41",
   "metadata": {},
   "source": [
    "## Better Solvers\n",
    "\n",
    "\n",
    "\n",
    "Good pattern matching guide for python - <https://peps.python.org/pep-0636/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acde0ea2-dd04-4083-bcf0-1bfc79620d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pwn\n",
    "from itertools import islice\n",
    "\n",
    "def solve_rng_beefy(\n",
    "    rng_stream,\n",
    "    p,\n",
    "    /,\n",
    "    max_L: int = int(1 << 10),\n",
    "    max_idx_samples: int = int(1 << 10),\n",
    "    gen_len: int | None = None,\n",
    ") -> int | None:\n",
    "    rng_stream_len = len(rng_stream)\n",
    "\n",
    "    with pwn.log.progress(\"Building rng_stream window dictionary\") as plog:\n",
    "        match LegendreRngDict.build_subview_dict(rng_stream, p, max_L=max_L, verbosity=0):\n",
    "            case None:\n",
    "                plog.failure(f\"key stream dict could not be built - loosen the {max_L=}\")\n",
    "                return None\n",
    "            case int(seed):\n",
    "                plog.success(f\"Key stream dictionary construction derived a seed-recovering overlap!\")\n",
    "                return seed\n",
    "            case LegendreRngDict() as rng_views:\n",
    "                plog.success(f\"Successfully built rng_stream window dictionary!\")\n",
    "                display(latex(rng_views))\n",
    "            case _ as res:\n",
    "                raise RuntimeError(\n",
    "                    f\"This should never happen - types should have exhaustively narrowed value into LegendreRngDict {res=}, {type(res)=}\"\n",
    "                )\n",
    "\n",
    "    with pwn.log.progress(\"Offset candidate trials\") as plog:\n",
    "\n",
    "        prev_cand = set()\n",
    "        for idx, view in rng_views.gen_overlaps_fast(gen_len=gen_len, max_idx_samples=max_idx_samples, skip_bloom=False):\n",
    "\n",
    "            if not (cand := (view.k_fp - idx)).degree():\n",
    "                plog.status(f\"Candidate view had zero-degree ({cand=})- dismissing...\")\n",
    "                continue\n",
    "\n",
    "            cand_seed = cand.any_root()\n",
    "            if cand_seed in prev_cand:\n",
    "                plog.status(f\"Repeated candidate found :-( - {cand_seed=}\")\n",
    "                continue\n",
    "\n",
    "            plog.status(f\"New candidate found! {cand_seed=}\")\n",
    "            if any(\n",
    "                exp != obs\n",
    "                for exp, obs\n",
    "                in zip(rng_stream, gen_rng_stream(p, cand_seed, rng_stream_len))\n",
    "            ):\n",
    "                prev_cand |= {cand_seed}\n",
    "                continue\n",
    "            \n",
    "            plog.success(f\"Candidate is a match! {cand_seed=}\")\n",
    "            display(view)\n",
    "            return cand_seed\n",
    "        else:\n",
    "            plog.failure(\"We could not find a solution! increase max_idx_samples/gen_len, or collect more bits\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930afda6-e908-4e5f-b185-8e9234ed4e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_chal_beefy(*args, **kwargs):\n",
    "    return solve_chal(*args, **kwargs, solver_func=solve_rng_beefy)\n",
    "\n",
    "if WITH_HEAVY_ASSERTS:\n",
    "    solve_chal_beefy(\n",
    "        queries=25, # 10_000, # 200, # 800,\n",
    "    \n",
    "        run_local_debug=True,\n",
    "        # ip=\"154.57.164.66\",\n",
    "        # port=31866,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b11bc82-f18f-4555-8638-b1517210df3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_size, p_bits = (\n",
    "    # 1 << 10, 20\n",
    "    # 1 << 15, 30\n",
    "    # 1 << 20, 40\n",
    "    # 1 << 25, 50 \n",
    "    #\n",
    "    # 1 << 11, 34\n",
    "    # \n",
    "    # 1 << 10, 30 \n",
    "    # 1 << 12, 40\n",
    "    # 1 << 14, 40\n",
    "    # 1 << 14, 48\n",
    "    # 1 << 14, 50\n",
    "    # 3 << 13, 48\n",
    "    # 3 << 13, 50\n",
    "    3 << 14, 56\n",
    "    #\n",
    "    # 1 << 15, 50\n",
    "    # 1 << 16, 50\n",
    "\n",
    "    # Above 1 << 14 bits of RNG material, the extracted substring dictionary actually hits python's\n",
    "    # memory threshold on my laptop, and construction speed massively drops from ~100_000 it/s to ~3 it/s :-(\n",
    "    # IDEA: use a probabilisitc filter (bloom, cuckoo, XOR, etc) to identify new elements quickly and w/o touching\n",
    "    # frequent lookups in the acutal massive subview dictionary.\n",
    "    # \n",
    "    # Split probabilstic filter and actual dictionary into two separate objects; the former for quick pre-emptive filtering,\n",
    "    # the latter for actual deep checks and view look-up when matches are found\n",
    ")\n",
    "\n",
    "display(\n",
    "    p := random_prime(1 << p_bits),\n",
    "    log(p, 2).n(),\n",
    "    k_secret := randint(1, p-1),\n",
    "    rng_size # := 1 << rng_bits\n",
    ")\n",
    "\n",
    "assert (\n",
    "    k_secret == (k_obs := solve_rng_beefy(\n",
    "        (rng_stream := tuple(tqdm(\n",
    "            gen_rng_stream(p, k_secret, rng_size),\n",
    "            total=int(rng_size),\n",
    "            desc=\"Creating simulated RNG stream\")\n",
    "        )),\n",
    "        p\n",
    "    ))\n",
    "), f\"{k_secret=}, {k_obs=}\"\n",
    "\n",
    "print(\"HOLDS!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4486725-efd0-4a5d-97e4-9cb71262c01f",
   "metadata": {},
   "source": [
    "# Ideas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea9efd9-ff40-4b39-b3ee-9781a5ff9299",
   "metadata": {},
   "outputs": [],
   "source": [
    "[_ for _ in tqdm(LegendreSampler(p=p, L=97, gen_len=1_000_000).gen_samples(10))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93499ff-24e6-4691-b343-587144d48df8",
   "metadata": {},
   "source": [
    "## Done\n",
    "\n",
    "- Try perhaps using a proper bit-level data structure?\n",
    "    - <https://doc.sagemath.org/html/en/reference/data_structures/sage/data_structures/bitset.html>\n",
    "    - <https://doc.sagemath.org/html/en/reference/data_structures/sage/data_structures/bounded_integer_sequences.html>\n",
    "- Other very cool sagemath data structures:\n",
    "    - <https://doc.sagemath.org/html/en/reference/data_structures/sage/data_structures/stream.html>\n",
    "    - <https://doc.sagemath.org/html/en/reference/data_structures/sage/misc/binary_tree.html>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f273f99-8a4a-4521-a632-a024d5a931e8",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "Legendre papers:\n",
    "- <https://eprint.iacr.org/2021/182.pdf>\n",
    "\n",
    "---\n",
    "\n",
    "Probabalistic filters:\n",
    "- <https://en.wikipedia.org/wiki/Bloom_filter>\n",
    "    - <https://www.geeksforgeeks.org/python/bloom-filters-introduction-and-python-implementation/>\n",
    "    - <https://systemdesign.one/bloom-filters-explained/>\n",
    "    - <https://hur.st/bloomfilter/>\n",
    "    - <https://github.com/KenanHanke/rbloom>\n",
    "        - <https://github.com/prashnts/pybloomfiltermmap3>\n",
    "        - <https://github.com/barrust/pyprobables>\n",
    "\n",
    "---\n",
    "\n",
    "- <https://en.wikipedia.org/wiki/Cuckoo_filter>\n",
    "    - <https://en.wikipedia.org/wiki/Cuckoo_hashing>\n",
    "\n",
    "- <https://lemire.me/blog/2019/12/19/xor-filters-faster-and-smaller-than-bloom-filters/>\n",
    "\n",
    "\n",
    "- <https://github.com/FastFilter>\n",
    "    - <https://github.com/FastFilter/pyfusefilter>\n",
    "    - <https://arxiv.org/abs/2201.01174>\n",
    "\n",
    "---\n",
    "\n",
    "- <https://web.stanford.edu/class/archive/cs/cs166/cs166.1216/>\n",
    "    - <https://web.stanford.edu/class/archive/cs/cs166/cs166.1216/lectures/13/Slides13.pdf>\n",
    "\n",
    " ---\n",
    "\n",
    "Cool things to look into\n",
    "\n",
    " - <https://en.wikipedia.org/wiki/MurmurHash#Vulnerabilities>\n",
    "     - <https://en.wikipedia.org/wiki/SipHash>\n",
    "     - <https://en.wikipedia.org/wiki/List_of_hash_functions#Non-cryptographic_hash_functions>\n",
    "\n",
    " \n",
    " - <https://doc.sagemath.org/html/en/reference/combinat/sage/combinat/bijectionist.html>\n",
    "\n",
    "     - <https://doc.sagemath.org/html/en/reference/combinat/sage/rings/cfinite_sequence.html>\n",
    "  \n",
    "---\n",
    "\n",
    "- <https://stackoverflow.com/questions/55824130/is-it-possible-to-create-a-minimal-perfect-hash-function-without-a-separate-look>\n",
    "    - <https://en.wikipedia.org/wiki/Perfect_hash_function>\n",
    "    - <https://stevehanov.ca/blog/?id=119>\n",
    "        - <https://cmph.sourceforge.net/>\n",
    "        - <https://engineering.indeedblog.com/blog/2018/02/indeed-mph/>\n",
    "        - <https://github.com/jermp/pthash>\n",
    "        - <https://github.com/rizkg/BBHash>\n",
    "            - <https://github.com/dib-lab/pybbhash>\n",
    "        - <https://github.com/ilanschnell/perfect-hash>\n",
    "    - <https://arxiv.org/html/2406.18099v1>\n",
    "    - n/a, but still cool - <https://github.com/codenotary/immudb>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305d1788-6a52-4095-a959-7bff8c967ce2",
   "metadata": {},
   "source": [
    "### Tasks:\n",
    "- <https://github.com/FastFilter> seems legit\n",
    "    - Experiment with <https://github.com/FastFilter/pyfusefilter>\n",
    "    - Consider using this to augment the current hash map approach\n",
    "\n",
    "sketched idea:\n",
    "- <https://github.com/KenanHanke/rbloom> for filtering during construction, since it is updatable and mutable\n",
    "    - Need to experiment with overlap-check-then-build, vs live building in batches, vs current live building and overlap checking\n",
    " \n",
    "\n",
    "- <https://github.com/FastFilter/pyfusefilter> for filtering once done, since it is immutable, size-initialisable, very memory efficient and very fast\n",
    "    - Can use this as the life candidate filter accelerator for making candidiate filtering as fast as possible\n",
    "    - <https://github.com/FastFilter/pyfusefilter/blob/c676938cb0a90e374a575f01d064cddda4024e0a/pyfusefilter/pyfusefilter.py#L7> is a little sus - what about forking it so it can accept bytes as input?\n",
    "        - Also, the closest I can get `bseq` to directly accessing its underlying data is `bytes()`, which still pads it out; perhaps I can build a child object that can expose its underlying integer representation as well?\n",
    "        - Correction - I think its hash is literally just `capacity * 1073807360 + data_in_lsb_order % ((((1 << 32) - 1) << 31) \\approx (1 << 62))`\n",
    "            - <https://github.com/sagemath/sage/blob/4015f9189b6aa95f46bb957e78d303f5fb20e77e/src/sage/data_structures/bounded_integer_sequences.pyx#L202>\n",
    "            - <https://github.com/sagemath/sage/blob/4015f9189b6aa95f46bb957e78d303f5fb20e77e/src/sage/data_structures/bitset_base.pxd#L630>\n",
    "        - Ergo, just find a way to do this with `hash(bseq)` as the actual input\n",
    "\n",
    "- Perhaps store the core table not as a hashmap, but as:\n",
    "\n",
    "    - A trie?\n",
    "        - <https://doc.sagemath.org/html/en/reference/combinat/sage/combinat/words/suffix_trees.html>\n",
    "    - A proper out-of-core database (e.g. <https://github.com/LMDB>)\n",
    "\n",
    "Also, for minimal overhead - why don't we move the subsequence comparison *into* the construction loop, so we don't even build a view if its substring doesn't match?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ff9438-36cf-4527-b58a-4ecc17f54b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install xxhash rbloom pyfusefilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12a953d-380a-43f0-b595-480c9e0debad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sage.data_structures.bounded_integer_sequences import BoundedIntegerSequence\n",
    "import xxhash\n",
    "\n",
    "# l_d = (M - 1) / L\n",
    "# expr = 2*M*l_d - ((L-1)*l_d*(l_d+1)) - 2**L\n",
    "# find_root(expr.subs(M=(size := 1 << 20)) == 0, 1, 100), log(2 * size ** (4/2)).n()\n",
    "\n",
    "# str(_bseq)\n",
    "# bytes(_bseq)\n",
    "# dumps(_bseq)\n",
    "MAGIC_NUMBER = 1073807360\n",
    "hash(BoundedIntegerSequence(2, [0,0,1])) - MAGIC_NUMBER, hash(BoundedIntegerSequence(2, [0,1,0])) - MAGIC_NUMBER\n",
    "\n",
    "ones, zeros = 32,31 # 31,32 # 1, 62\n",
    "display(\n",
    "    hash(BoundedIntegerSequence(2,  ([0] * zeros) + ([1] * ones))) - 1073807360 - (((1 << ones) - 1) << zeros),\n",
    "    xxhash.xxh64_intdigest(hash(_bseq).to_bytes(8)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008bb095-c1eb-47a0-a5a9-7ea194b3d085",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rbloom import Bloom\n",
    "\n",
    "Bloom(10_000_000, 0.0001)\n",
    "\n",
    "\n",
    "import pyfusefilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabce9c3-dc22-429e-b4c3-3785df53c7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seed(31337)\n",
    "dict_lrng = LegendreRngDict.build_subview_dict(gen_rng_stream(p, k_secret, 1 << 14), p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01db88c-c0a9-48ce-a27d-28ff62babd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install Pympler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b843675-e96e-4674-a911-53bec13c6f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pympler.asizeof import *\n",
    "\n",
    "asizeof(dict_lrng), 1_553_050_920"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bad3c4b-b61a-4a9e-bc14-89752093346e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "from pympler.asizeof import asized\n",
    "v_example = list(islice(dict_lrng.values(), 10))\n",
    "asized(v_example, detail=3).format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bdbff8-3344-4dba-be70-55e3c3235116",
   "metadata": {},
   "outputs": [],
   "source": [
    "del dict_lrng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc534430-6950-4e2d-8b6f-be26639e6002",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit (fuse_filter := pyfusefilter.Fuse16(dict_lrng.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def82f2d-feed-4717-b9e9-ed5a8426be2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fuse_filter\n",
    "# pyfusefilter.Fuse16(tqdm(dict_lrng.keys()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMath 10.7",
   "language": "sage",
   "name": "sagemath"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
